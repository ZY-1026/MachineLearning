{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性回归中使用梯度下降"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(666)\n",
    "x = 2 * np.random.random(size=100)\n",
    "y = x * 3. + 4. + np.random.normal(size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = x.reshape(-1, 1)\n",
    "X.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用梯度下降法训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 求损失函数\n",
    "def J(theta, X_b, y):\n",
    "    try:\n",
    "        return np.sum((y - X_b.dot(theta)) ** 2) / len(X_b)\n",
    "    except:\n",
    "        return float(\"inf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 求偏导数\n",
    "def dJ(theta, X_b, y):\n",
    "    res = np.empty(len(theta))\n",
    "    res[0] = np.sum(X_b.dot(theta) - y)\n",
    "    for i in range(1, len(theta)):\n",
    "        res[i] = (X_b.dot(theta) - y).dot(X_b[:,i])\n",
    "    return res * 2 / len(X_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 利用梯度下降求解\n",
    "def gradient_descent(X_b, y, initial_theta, eta, n_itera= 1e4, epsilon=1e-8):\n",
    "    theta = initial_theta\n",
    "    i_iter = 0\n",
    "    while i_iter < n_itera:\n",
    "        gradient = dJ(theta, X_b, y)\n",
    "        last_theta = theta\n",
    "        theta = theta - eta * gradient\n",
    "        if abs(J(theta, X_b, y) - J(last_theta, X_b, y)) < epsilon:\n",
    "            break\n",
    "        i_iter += 1\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_b = np.hstack([np.ones((len(x), 1)), x.reshape(-1,1)])\n",
    "initial_theta = np.zeros(X_b.shape[1])\n",
    "eta = 0.01\n",
    "\n",
    "theta = gradient_descent(X_b, y, initial_theta, eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.02145786, 3.00706277])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 梯度下降的向量法实现方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = datasets.load_boston()\n",
    "\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "\n",
    "X = X[y < 50.0]\n",
    "y = y[y < 50.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from playML.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, seed=666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8129794056212711"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from playML.LinearRegression import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "%time lin_reg.fit_normal(X_train, y_train)\n",
    "lin_reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用梯度下降"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\softwares\\python\\python36\\lib\\site-packages\\numpy\\core\\fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "E:\\Workspace\\Jupyter\\ML\\Gradient\\playML\\LinearRegression.py:32: RuntimeWarning: overflow encountered in square\n",
      "  return np.sum(y - X_b.dot(theta) ** 2) / len(X_b)\n",
      "E:\\Workspace\\Jupyter\\ML\\Gradient\\playML\\LinearRegression.py:51: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if abs(J(theta, X_b, y) - J(last_theta, X_b, y)) < epsilon:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_reg2 = LinearRegression()\n",
    "\n",
    "lin_reg2.fit_gd(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_reg2.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.42362e+01, 0.00000e+00, 1.81000e+01, 0.00000e+00, 6.93000e-01,\n",
       "        6.34300e+00, 1.00000e+02, 1.57410e+00, 2.40000e+01, 6.66000e+02,\n",
       "        2.02000e+01, 3.96900e+02, 2.03200e+01],\n",
       "       [3.67822e+00, 0.00000e+00, 1.81000e+01, 0.00000e+00, 7.70000e-01,\n",
       "        5.36200e+00, 9.62000e+01, 2.10360e+00, 2.40000e+01, 6.66000e+02,\n",
       "        2.02000e+01, 3.80790e+02, 1.01900e+01],\n",
       "       [1.04690e-01, 4.00000e+01, 6.41000e+00, 1.00000e+00, 4.47000e-01,\n",
       "        7.26700e+00, 4.90000e+01, 4.78720e+00, 4.00000e+00, 2.54000e+02,\n",
       "        1.76000e+01, 3.89250e+02, 6.05000e+00],\n",
       "       [1.15172e+00, 0.00000e+00, 8.14000e+00, 0.00000e+00, 5.38000e-01,\n",
       "        5.70100e+00, 9.50000e+01, 3.78720e+00, 4.00000e+00, 3.07000e+02,\n",
       "        2.10000e+01, 3.58770e+02, 1.83500e+01],\n",
       "       [6.58800e-02, 0.00000e+00, 2.46000e+00, 0.00000e+00, 4.88000e-01,\n",
       "        7.76500e+00, 8.33000e+01, 2.74100e+00, 3.00000e+00, 1.93000e+02,\n",
       "        1.78000e+01, 3.95560e+02, 7.56000e+00],\n",
       "       [2.49800e-02, 0.00000e+00, 1.89000e+00, 0.00000e+00, 5.18000e-01,\n",
       "        6.54000e+00, 5.97000e+01, 6.26690e+00, 1.00000e+00, 4.22000e+02,\n",
       "        1.59000e+01, 3.89960e+02, 8.65000e+00],\n",
       "       [7.75223e+00, 0.00000e+00, 1.81000e+01, 0.00000e+00, 7.13000e-01,\n",
       "        6.30100e+00, 8.37000e+01, 2.78310e+00, 2.40000e+01, 6.66000e+02,\n",
       "        2.02000e+01, 2.72210e+02, 1.62300e+01],\n",
       "       [9.88430e-01, 0.00000e+00, 8.14000e+00, 0.00000e+00, 5.38000e-01,\n",
       "        5.81300e+00, 1.00000e+02, 4.09520e+00, 4.00000e+00, 3.07000e+02,\n",
       "        2.10000e+01, 3.94540e+02, 1.98800e+01],\n",
       "       [1.14320e-01, 0.00000e+00, 8.56000e+00, 0.00000e+00, 5.20000e-01,\n",
       "        6.78100e+00, 7.13000e+01, 2.85610e+00, 5.00000e+00, 3.84000e+02,\n",
       "        2.09000e+01, 3.95580e+02, 7.67000e+00],\n",
       "       [5.69175e+00, 0.00000e+00, 1.81000e+01, 0.00000e+00, 5.83000e-01,\n",
       "        6.11400e+00, 7.98000e+01, 3.54590e+00, 2.40000e+01, 6.66000e+02,\n",
       "        2.02000e+01, 3.92680e+02, 1.49800e+01]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:10, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_reg2.fit_gd(X_train, y_train, eta=0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27586818724477224"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_reg2.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.1 s ± 388 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit lin_reg2.fit_gd(X_train, y_train, eta=0.000001, n_iters=1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7542932581943915"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_reg2.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 在梯度下降之前对数据进行归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standardScaler = StandardScaler()\n",
    "standardScaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_standard = standardScaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 260 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_reg3 = LinearRegression()\n",
    "%time lin_reg3.fit_gd(X_train_standard, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_standard = standardScaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8129794246544336"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_reg3.score(X_test_standard, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 梯度下降法的优势"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 1000\n",
    "n = 5000\n",
    "\n",
    "big_X = np.random.normal(size = (m,n))\n",
    "\n",
    "true_theta = np.random.uniform(0.0, 100.0, size=n+1)\n",
    "\n",
    "big_y = big_X.dot(true_theta[1:]) + true_theta[0] + np.random.normal(0., 10., size=m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.68 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_reg1 = LinearRegression()\n",
    "%time big_reg1.fit_normal(big_X, big_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9.23 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_reg2 = LinearRegression()\n",
    "%time big_reg2.fit_gd(big_X, big_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
